
# Fine-Tune Large Language Model


## Reinforcement Learning from Human Feedback (RLHF)




| Type | Model  | Use Case |  
| --- | --- | --- | 
| Encoder | autoEncoder | sentiment analysis | 
| Encoder + Decoder | sequence-2-sequence | translation, summarization, Q & A |
| Decoder | autoregression | GPT | 



## How Do Large Language Models Work?

The most well-known Large Language Model (LLM) architecture is the **transformer architecture** shown below (picture from the paper **Attention Is All You Need** [[Ashish Vaswani et al.]][Attention Is All You Need]). 

![transformer](images/transformer.png)


* [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1]: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/#:~:text=Positional%20encoding%20describes%20the%20location,item%27s%20position%20in%20transformer%20models.
[[Jason Brownlee] A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/#:~:text=Positional%20encoding%20describes%20the%20location,item%27s%20position%20in%20transformer%20models.)
* [What is the positional encoding in the transformer model?]: https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model
[[Data Science StackExchange] What is the positional encoding in the transformer model?](https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model)



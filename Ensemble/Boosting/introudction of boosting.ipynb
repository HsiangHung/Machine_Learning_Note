{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms in Machine Learning\n",
    "\n",
    "Boosting algorithms are one of the most widely used algorithm in data science competitions. In many data hackathon competitions, many experts agree that boosting algorithm is able to improve accuracy of their models.\n",
    "\n",
    "Here we just walk through the concepts and explain how the boosting algorithms work. There are seversal excellent **Analytics Vidhya** tutorial blogs which I follow and learn from. They are [Quick Introduction to Boosting Algorithms in Machine Learning](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/), [Getting smart with Machine Learning – AdaBoost and Gradient Boost](https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/) and [Learn Gradient Boosting Algorithm for better predictions (with codes in R)](https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/) and [Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/).\n",
    "\n",
    "### What is Boosting?\n",
    "\n",
    "The term `Boosting` refers to a family of algorithms which converts a set of weak learners to a single strong learner.\n",
    "\n",
    "### How Boosting Algorithms works?\n",
    "\n",
    "An immediate question which should pop in your mind is, `How boosting identify weak rules?`\n",
    "\n",
    "To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
    "\n",
    "Here’s another question which might haunt you, `How do we choose different distribution for each round?`\n",
    "\n",
    "Step 1:  The base learner takes all the distributions and assign **equal weight or attention to each observation**.\n",
    "\n",
    "Step 2: If there is any **prediction error** caused by first base learning algorithm, then we pay **higher** attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model. **Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules**.\n",
    "\n",
    "![title](boosting_illustration.png)\n",
    "\n",
    "**D1**: The decision stump (D1) has generated vertical line at left side to classify the data points. We see that, this vertical line has incorrectly predicted three + (plus) as – (minus). In such case, we’ll assign higher weights to these three + (plus) and apply another decision stump. After reassigning, you can see that the size of three incorrectly predicted + (plus) is bigger as compared to rest of the data points\n",
    "\n",
    "**D2**: Now, a vertical line (D2) at right side of this box has classified three mis-classified + (plus) correctly. But again, it has caused mis-classification errors. This time with three -(minus). Again, we will assign higher weight to three – (minus) and apply another decision stump, and after assigning, three – (minus) are given higher weights.\n",
    "\n",
    "**D3**: A decision stump (D3) is applied to predict these mis-classified observation correctly. This time a horizontal line is generated to classify + (plus) and – (minus) based on higher weight of mis-classified observation.\n",
    "\n",
    "**Finally**: we have combined D1, D2 and D3 to form a strong prediction having complex rule as compared to individual weak learner. You can see that this algorithm has classified these observation quite well as compared to any of individual weak learner.\n",
    "\n",
    "### A very clear high level math illustration for how boosting works\n",
    "\n",
    "One simple way is to build an entirely different model using new set of input variables and trying better ensemble learners. On the contrary, I have a much simpler way to suggest. It goes like this:\n",
    "$$Y =M(X)+\\epsilon$$\n",
    "What if I am able to see that **error is not a white noise but have same correlation with outcome(Y) value**. What if we can develop a model on this error term as\n",
    "$$Y =M(X)+ \\big( G(x) + \\epsilon_2 \\big)$$\n",
    "and then\n",
    "$$Y =M(X)+ G(x) + \\big(H(x) + \\epsilon_3 \\big)$$\n",
    "\n",
    "What if I can find an optimal weights for each of the three learners, $M(X), G(x), H(x)$ such that\n",
    "\n",
    "$$Y =\\alpha M(X)+ \\beta G(x) + \\gamma H(x) + \\epsilon_4$$\n",
    "\n",
    "There are two quick questions:\n",
    "\n",
    "1. Do we really see non white noise error in regression/classification equations? If not, how can we even use this algorithm?\n",
    "\n",
    "2. If this is possible, why not get near 100% accuracy?\n",
    "\n",
    "Boosting is generally done on weak learners, which do not have a capacity to leave behind white noise.  Secondly, boosting can lead to overfitting, so we need to stop at the right point.\n",
    "\n",
    "### Reweighting data\n",
    "\n",
    "Ok. Then how can we assign weights to misclassified data? Here are the processes.\n",
    "\n",
    "Given data, $(x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n)$ where $x_i \\in {\\bf X}_i$ and $y_i \\in \\lbrace 1,-1 \\rbrace$. At the beginning, all data points have equal weights. So we initialize $D_1(i)=1/n$ for $i=1,2,\\cdots, n$. \n",
    "\n",
    "Supposed at the $t$-th iteration, the classifier learner $h_t$ give prediction $\\hat{y}_i = h_t(x_i)$ and the corresponding data weight is $D_t(i)$, each data weight is updated as \n",
    "\n",
    "$$D_{t+1}(i) = \\frac{D_t(i)\\exp{\\big(-\\alpha_t y_i \\hat{y}_i \\big)}}{Z_t},$$\n",
    "\n",
    "where $Z_t$ is a normalization constant and $\\alpha_t$ is to be determined. Under such construction, the final classifier is equal to a collection of weighted classifiers $\\lbrace \\alpha_1 h_1, \\alpha_2 h_2,\\cdots \\alpha_T h_T \\rbrace$\n",
    "$$H(x) = sgn\\big( \\sum^T_{t=1}\\alpha_t h_t(x)\\big)$$\n",
    "and $\\alpha_t$ should be\n",
    "$$\\alpha_t = \\frac{1}{2}\\ln \\Big( \\frac{1-\\epsilon_t}{\\epsilon_t}\\Big).$$\n",
    "\n",
    "$\\epsilon_t$ is the weighted training error:\n",
    "$$\\epsilon_t = P_{i\\sim D_t(i)}(h_t(x_i) \\ne y_i) = \\sum^n_{i=1} D_t(i) \\delta_{\\hat{y}_i, -y_i},$$\n",
    "where when $h_t(x_i)=\\hat{y}_i \\ne y_i$, i.e. when prediction is wrong, $\\delta_{\\hat{y}_i, -y_i} = 1$. Thus $\\epsilon_t$ is the aggregated weighted training error.\n",
    "\n",
    "Without weighting scheme, the accuracy for a single classifier is\n",
    "$$I = \\sum_{i} {\\bf I}(\\hat{y}_i=y_i).$$\n",
    "With the weights, the metric accuracy turns out to be\n",
    "$$I= \\sum_i D_t(i) {\\bf I}(\\hat{y}_i=y_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost (Adaptive Boosting)\n",
    "\n",
    "It works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\n",
    "\n",
    "Mostly, we use decision stamps with AdaBoost. But, `we can use any machine learning algorithms as base learner if it accepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.ensemble import AdaBoostRegressor #For Regression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier() \n",
    "clf = AdaBoostClassifier(n_estimators=100, base_estimator=dt,learning_rate=1)\n",
    "#Above I have used decision tree as a base estimator, you can use any ML learner as base estimator if it ac# cepts sample weight \n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters for tuning are:\n",
    "\n",
    "**n_estimators**: It controls the number of weak learners.\n",
    "\n",
    "**learning_rate**:Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "**base_estimators**: It helps to specify different ML algorithm.\n",
    "\n",
    "You can also tune the parameters of base learners to optimize its performance.\n",
    "\n",
    "## Boosting Algorithm: Gradient Boosting\n",
    "\n",
    "In gradient boosting, it trains many model sequentially. Each new model gradually minimizes the loss function (y = ax + b + e, e needs special attention as it is an error term) of the whole system using Gradient Descent method. The learning procedure consecutively fit new models to provide a more accurate estimate of the response variable.\n",
    "\n",
    "The principle idea behind this algorithm is to construct new base learners which can be maximally correlated with negative gradient of the loss function, associated with the whole ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier #For Classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor #For Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are\n",
    "\n",
    "**n_estimators**: It controls the number of weak learners.\n",
    "\n",
    "**learning_rate**: Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "**max_depth**: maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "\n",
    "You can tune loss function for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
